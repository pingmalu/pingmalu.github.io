---
layout: post
title: 超参优化
---
# optuna

文档：[https://optuna.readthedocs.io/](https://optuna.readthedocs.io/)

## 性能优化

### 分布式性能优化

TPESampler减少参数重复的策略：

1.使用 Constant Liar 策略

`constant_liar` 是 `TPESampler` 的一个参数，当设置为 `True` 时，会在分布式环境中减少参数重复的情况。当一个试验开始但尚未完成时，Constant Liar 策略会假设该试验的目标值为“常数”（“lie”）。这样做可以在参数空间中“占位”，让后续的采样趋向于选择不同的参数。该策略特别适用于每次评估代价较高的情况，可以有效减少不同工作节点选择重复参数的可能性。

2.增大 n_startup_trials

  `n_startup_trials` 参数控制在开始使用TPE算法之前，Optuna 会进行多少次随机采样。通过增大这个数值，可以在初始阶段增加参数的多样性，间接减少后续过程中参数选择的重复性。

3.增大 n_ei_candidates

从理论上讲，增加 `n_ei_candidates` 的值会使得算法在决定每一步的参数值时有更多的候选点考虑，这可能会增加选择不同参数的可能性，从而在一定程度上减少在分布式环境下的参数重复。

### 寻参函数改进

optuna在分布式超参优化场景下，采样器对重复的trial参数会用竞争策略，这将导致集群环境下重复评估，训练高负载和全域参数空间较小情况下浪费大量算力。

改造下寻参函数，将重复入参COMPLETE状态直接取值跳过训练过程，RUNNING状态直接返回空（optuna会将空置为Fail状态）：

```python
# 求最优超参数
def f(trial):
    # 定义要找的超参数,并设置上下限
    params = {
        'minima_size': trial.suggest_int('minima_size', args.minima_size_a, args.minima_size_b,step=minima_size_STEP),
    }

    print()
    t_list = trial.study.get_trials(states=[TrialState.COMPLETE,TrialState.RUNNING])
    n = trial._cached_frozen_trial
    for t in t_list:
        if n.number == t.number:
            continue
        if n.params == t.params:
            if 'HOST' in t.user_attrs and 'host' in t.user_attrs['HOST']:
                host_s = t.user_attrs['HOST']['host']
            else:
                host_s = ""
            HOST_dic = {"host": socket.gethostname(),"msg":None,"number": t.number, "HOST_s": host_s}
            if t.state == TrialState.RUNNING:
                HOST_dic["msg"] = "RUNNING"
                print("超参重复将跳过",t.params,HOST_dic)
                trial.set_user_attr("HOST", HOST_dic)
                return
                # raise optuna.TrialPruned # 这里不再剪枝
            else:
                HOST_dic["msg"] = "COMPLETE"
                print("超参重复将跳过",t.params,HOST_dic)
                trial.set_user_attr("HOST", HOST_dic)
                return t.value
                # raise optuna.TrialPruned # 这里不再剪枝
    trial.set_user_attr("HOST", {"host": socket.gethostname()})

    # 评估超参数
    return _c.params_test(args,params,MC)
```

## 网格搜索使用步进函数

网格搜索需要定义搜索空间，遇到连续枚举时，用步进函数替代它：

```python
def generate_float_list(start, end, step):
    """生成一个从 start 到 end 的列表，步长为 step。

    Args:
        start: 列表的起始值。
        end: 列表的结束值。
        step: 列表的步长。

    Returns:
        一个从 start 到 end 的列表，步长为 step。
    """

    list = []
    current = start
    while current <= end:
        list.append(current)
        current += step
    return list

search_space = {"minima_size":generate_float_list(args.minima_size_a, args.minima_size_b,step=minima_size_STEP)}

print("优化器:",study_name)

study = optuna.create_study(
    sampler=GridSampler(search_space),
    study_name=study_name,
    direction='maximize',
    load_if_exists=True,
    storage=args.sqluri
    )
```


## 优化退出

优化过程中直接退出会导致任务一直处于`Running`状态，可以通过接收信号后抛出OptunaError异常来让主优化器修改状态，另外可以通过捕获`SIGINT`来让主优化器在当前训练结束后再停止，

**使用信号处理**：捕获如`SIGINT`或`SIGTERM`等信号，并定义一个处理函数来优雅地终止Optuna的优化过程。如：

``` python
import signal
import optuna

study = optuna.create_study()

def signal_handler(sig, frame):
	print('收到立即停止优化的信号.')
	args.optuna_stop = True  # 全局停止 用于循环体中退出
	study.stop()             # 优化器停止
	raise optuna.exceptions.OptunaError("立即中断当前训练.")

def signal_handler_c(sig, frame):
	print('收到优雅停止优化的信号.ctrl+c')
	args.optuna_stop = True  # 全局停止 用于循环体中退出
	study.stop()             # 优化器停止

# 绑定信号处理器
signal.signal(signal.SIGTERM, signal_handler)    # kill
signal.signal(signal.SIGINT, signal_handler_c)   # ctrl+c

study.optimize(objective, n_trials=100)
```

